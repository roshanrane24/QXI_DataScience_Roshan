# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10gdn46qto0zgHzu8JKsAqOoRI_GI9yUq
"""

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision.datasets import MNIST
import torchvision.transforms as transforms


class CNN_MNIST(nn.Module):
    def __init__(self, lr, epochs, batch_size, num_classes=10):
        super(CNN_MNIST, self).__init__()
        self.val_loss = []
        self.val_acc = []
        self.epoch_loss = []
        self.epoch_accuracy = []
        self.lr = lr
        self.epochs = epochs
        self.batch_size = batch_size
        self.num_classes = num_classes

        # Selecting which device to compute on
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

        # Defining transformation to be done on image
        self.transform = transforms.Compose([transforms.ToTensor()])

        # Defining Network layer
        self.features = nn.Sequential(
            # first convolution layer
            nn.Conv2d(1, 400, 3, 1, 1),
            nn.BatchNorm2d(400),
            nn.ReLU(),

            # second convolution layer
            nn.Conv2d(400, 200, 3, 1, 1),
            nn.MaxPool2d(2),
            nn.ReLU(),

            # third convolution layer
            nn.Conv2d(200, 100, 3, 1, 1),
            nn.Conv2d(100, 50, 3, 1, 1),
            nn.ReLU()
        )

        clf_in_dim = self.calc_dim()

        self.classifier = nn.Sequential(
            nn.Linear(clf_in_dim, 25),
            nn.ReLU(),
            nn.Linear(25, 12),
            nn.ReLU(),
            nn.Linear(12,self.num_classes)
        )

        self.loss = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.parameters(), lr=lr)

        self.to(self.device)

        self.load_data()

    # calc dim function
    def calc_dim(self):
        data = torch.zeros((1, 1, 28, 28))
        data = self.features(data)

        return int(np.prod(data.size()))

    # Loading Data sets
    def load_data(self):
        indices = np.arange(60000)
        np.random.shuffle(indices)

        train_set = MNIST(root="mnist",
                          train=True,
                          download=True,
                          transform=self.transform)

        test_set = MNIST(root='mnist',
                         train=False,
                         download=True,
                         transform=self.transform)

        self.train_loader = torch.utils.data.DataLoader(train_set,
                                                        batch_size=self.batch_size,
                                                        shuffle=False,
                                                        num_workers=4,
                                                        sampler=torch.utils.data.RandomSampler(indices[:50000]))
        self.val_loader = torch.utils.data.DataLoader(train_set,
                                                      batch_size=self.batch_size,
                                                      shuffle=False,
                                                      num_workers=4,
                                                      sampler=torch.utils.data.RandomSampler(indices[50000:]))
        self.test_loader = torch.utils.data.DataLoader(test_set,
                                                       batch_size=self.batch_size,
                                                       shuffle=True,
                                                       num_workers=4)

    def forward(self, data):
        # data = torch.tensor(data).to(self.device)

        data = self.features(data)

        data = data.view(data.size()[0], -1)

        results = self.classifier(data)

        return results

    def train_model(self):
        for epoch in range(1, self.epochs + 1):
            self.train()
            print(f'\033[1mEpoch {epoch:d}/{self.epochs:d}\033[0m'.center(50, '#'))
            ep_loss = 0
            ep_acc = []

            update = list(range(0, 10000, 1000))

            for i, (inputs, labels) in enumerate(self.train_loader):
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                self.optimizer.zero_grad()

                predictions = self.forward(inputs)

                loss = self.loss(predictions, labels)
                _, predictions = torch.max(predictions, 1)
                correct = (predictions == labels).sum().item()

                acc = correct / self.batch_size
                loss.backward()
                self.optimizer.step()
                if i in update:
                    self.epoch_loss.append(loss)
                    self.epoch_accuracy.append(acc)
                    print(f"Loss: {loss:3f} |    Accuracy: {100*acc:2f}%".ljust(50))
                self.epoch_loss.append(loss)
                self.epoch_accuracy.append(acc)
                ep_loss = loss
                ep_acc.append(acc)

            print(f"\033[1mEpoch Loss: {ep_loss:3f}    |    Epoch Accuracy: {100*np.mean(ep_acc):2f}%\033[0m".center(50))
            self.val_test()

    def val_test(self):
        self.eval()
        vl = 0
        va = []
        with torch.no_grad():
            for i, (inputs, labels) in enumerate(self.val_loader):
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                
                predictions = self.forward(inputs)
                
                loss = self.loss(predictions, labels)
                _, predictions = torch.max(predictions, 1)
                correct = (predictions == labels).sum().item()

                acc = correct / self.batch_size
                self.val_loss.append(loss)
                self.val_acc.append(acc)
                vl = loss
                va.append(acc)
        print(f"\033[1mValidation Loss: {vl:3f}  |    Validation Accuracy: {100*np.mean(va):2f}%\033[0m".center(50))

    def test_model(self):
        self.eval()
        test_loss = 0
        test_acc = []
        with torch.no_grad():
            for i, (inputs, labels) in enumerate(self.test_loader):
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                predictions = self.forward(inputs)

                loss = self.loss(predictions, labels)
                
                loss = self.loss(predictions, labels)
                _, predictions = torch.max(predictions, 1)
                correct = (predictions == labels).sum().item()

                acc = correct / self.batch_size
                test_loss = loss
                test_acc.append(acc)
        print(f"Test Loss: {test_loss:3f}   |    Test Accuracy: {np.mean(100*test_acc):2f}%".center(50))


if __name__ == '__main__':
    cnn = CNN_MNIST(0.0001, 5, 32, 10)
    cnn.train_model()

    plt.figure(figsize=(10.3,6))
    plt.plot(cnn.epoch_loss, color='orange',alpha=0.8)
    # plt.title('Loss')
    
    # plt.figure(figsize=(10.3,6))
    plt.plot(cnn.epoch_accuracy, color='green',alpha=0.8)
    plt.title('Training')
    plt.show()
# Validation
    plt.figure(figsize=(10.3,6))
    plt.plot(cnn.val_loss, color='orange',alpha=0.8)
    # plt.title('Loss')
    
    # plt.figure(figsize=(10.3,6))
    plt.plot(cnn.val_acc, color='green',alpha=0.8)
    plt.title('Validation')
    plt.show()

    cnn.test_model()